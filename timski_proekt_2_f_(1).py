# -*- coding: utf-8 -*-
"""Timski_proekt_2_F (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-2FFfpq_zEm1GnqqI-altnSFId8_8s7Q
"""

!pip install wfdb
!pip install detecta
!pip install pyocclient

import wfdb
import os
import numpy as np
import pandas as pd
from detecta import detect_peaks
from scipy import signal
import math
import owncloud
import pickle

#windows
w_flat = 3;    # flat lines window
w_peaks =4;    # flat peaks window
w_fix = 14;     # flat join window

# thresholds
t_peaks = 0.1; # percentage of tolerated flat peaks
t_flat = 0.1;  # percentage of tolerated flat lines

#sampling rate
fsppg = 500

def flat_lines(data=None, window=None):
    # Flat line in ABP and PPG -> sliding window over the whole thing
    len = np.size(data)
    flat_locs_ppg = np.ones((len - window), dtype=int)

    # efficient-ish sliding window
    for i in range(2, window):
        tmp_ppg = data[0:(len - window)] == data[i:(len - window + i)]
        flat_locs_ppg = np.logical_and(flat_locs_ppg, tmp_ppg)

    # extend to be the same size as data
    flat_locs_ppg = np.concatenate([flat_locs_ppg, np.zeros(window)])

    flat_locs_ppg2 = flat_locs_ppg

    for i in range(2, window):
        flat_locs_ppg[i:] = np.logical_or(flat_locs_ppg[i:], flat_locs_ppg2[1:(np.size(flat_locs_ppg2) - i + 1)])

    per_ppg = np.sum(flat_locs_ppg) / len
    return per_ppg


def flat_peaks(data=None, window=None):
    # ppg_peaks   ... peak locations for PPG
    ppg_peaks = detect_peaks(data[0:], show=False)
    # ppg_valleys ... cycle start/end points for PPG
    ppg_valleys = detect_peaks(-1 * data[0:], show=False)

    number_of_peaks_ppg = np.size(ppg_peaks)
    number_of_valleys_ppg = np.size(ppg_valleys)
    # first get the flat lines:
    len = np.size(data)
    flat_locs_ppg = np.ones((len - window), dtype=int)

    # get the locations where i == i+1 == i+2 ... == i+window
    # efficient-ish sliding window
    for i in range(2, window):
        tmp_ppg = data[0:(len - window)] == data[i:(len - window + i)]
        flat_locs_ppg = np.logical_and(flat_locs_ppg, tmp_ppg)

    # extend to be the same size as data
    flat_locs_ppg = np.concatenate([flat_locs_ppg, np.zeros(window)])

    ppg_peak_ones = np.zeros(np.size(data[0:]))
    ppg_peak_ones[ppg_peaks] = 1;
    ppg_valley_ones = np.zeros(np.size(data[0:]))
    ppg_valley_ones[ppg_valleys] = 1;

    # extract the needed info:
    locs_of_flat_peaks_ppg = np.flatnonzero(np.logical_and(flat_locs_ppg, ppg_peak_ones));
    locs_of_flat_valleys_ppg = np.flatnonzero(np.logical_and(flat_locs_ppg, ppg_valley_ones));
    number_of_flat_peaks_ppg = np.size(locs_of_flat_peaks_ppg);
    number_of_flat_valleys_ppg = np.size(locs_of_flat_valleys_ppg);

    skip_ppg = 0;

    if (number_of_flat_peaks_ppg >= t_peaks * number_of_peaks_ppg) or (
            number_of_flat_valleys_ppg >= t_peaks * number_of_valleys_ppg):
        skip_ppg = 1

    return skip_ppg


def filter_signal(data=None):
    window_size = 100
    window = np.ones(window_size) / float(window_size)
    filtered_signal = np.convolve(data, window, mode='same')
    return filtered_signal

np.set_printoptions(threshold=np.inf)
n=0
rows_list = []
rows_list2 = []
# Reading all record names of the database and sorting them
data=wfdb.io.get_dbs()
url='pulse-transit-time-ppg'
np.set_printoptions(threshold=np.inf)
list=wfdb.get_record_list(url) #list of all records
# Iterating through the record names list
for i in list[:]:
    # display(i)
  for j in range(1, 7):
      n+=1
      #globals variable
      signals=[f'signals{j}']
      fields=[f'fields{j}']
      result=[f'result{j}']
      pleth=[f'pleth{j}']

      print(i," Pleth:",j)
      #Reading the corresponding physiologic waveform signals header
      signals,fields = wfdb.rdsamp(i, pn_dir=f'{url}/',channels=[j])
      # display(signals)
      # display(fields)
      #getting value of comments from fields
      value = fields['comments']
      #getting first value
      values_str = value[0]
      #separation from the value found - spo2
      spo2_end_str = values_str.split('<spo2_end>: ')[1]
      spo2_end = int(spo2_end_str.split()[0])
      #finding and taking the length of the signal from the field
      sig_len = fields['sig_len']
      pleth=signals
      # display(fields)
      #checking if the length of the signal is less than 0
      if len(pleth) > 0:
        # print("There is a PLETH signal information")

        # Printing the lengths of the corresponding PPG signals
        print("Length of PPG signal: " + str(sig_len))
        counter = 0
        percentage_saved = 0

        # Normalization of the whole PPG record
        ppg_signal = (pleth - np.nanmin(pleth)) / (np.nanmax(pleth) - np.nanmin(pleth))

        #Detecting flat lines in the signal
        data = ppg_signal.reshape(np.size(ppg_signal), )
        p_ppg = flat_lines(data, w_flat)

        if (p_ppg < t_flat):
          print("flat_line: "+ str(float(p_ppg)))
          #Detecting flat peaks in the signal
          skip_ppg = flat_peaks(data, w_peaks)
          print("flat_peak: "+ str(float(skip_ppg)))

          if skip_ppg == 0:
            #removing noise at the beginning and at the end of the segment
            data = filter_signal(data)
            print("Signal is good!")

            ppg_signal = data
            # ppg_sig_str = ','.join(map(str, ppg_signal))
            # Building and saving the dictionary filtered objects
            dictionary = {
              "ID": i,
              "PPG": ppg_signal,
              "Pleth":j,
              "SpO2": spo2_end
            }

            percentage_saved += 1
            # display(dictionary)
            rows_list.append(dictionary)

            result = [elem[0] for elem in pleth]
            pleth_ = ','.join(map(str, result))

            # print("SpO2:",spo2_end)
            # Building and saving the dictionary without filtered objects
            #PPG-> signalot(s1_walk) , Pleth-> signalot od 1..6
            dict = {"ID": i,"PPG": pleth_,"Pleth":j,"SpO2": spo2_end}
            rows_list2.append(dict)


            # Calculating and printing the percentage of data saved from the corresponding record
            percentage_saved = percentage_saved / len(ppg_signal) * 100
            # print(str(percentage_saved) + "% of all segments of the record were saved.")
        else:
          print("Ð¢he signal is rejected!!!")
      print("=============================")

df = pd.DataFrame(rows_list)
df2 = pd.DataFrame(rows_list2)
# df.to_csv('filtered-database.csv', index=False)

# np.set_printoptions(threshold=np.inf)
# n=0
# rows_list = []
# rows_list2 = []
# # Reading all record names of the database and sorting them
# data=wfdb.io.get_dbs()
# url='pulse-transit-time-ppg'
# np.set_printoptions(threshold=np.inf)
# list=wfdb.get_record_list(url) #list of all records
# # Iterating through the record names list
# for i in list[:]:
#     # display(i)
#   for j in range(1, 7):
#       n+=1
#       #globals variable
#       signals=[f'signals{j}']
#       fields=[f'fields{j}']
#       result=[f'result{j}']
#       pleth=[f'pleth{j}']

#       print(i," Pleth:",j)
#       #Reading the corresponding physiologic waveform signals header
#       signals,fields = wfdb.rdsamp(i, pn_dir=f'{url}/',channels=[j])
#       # display(signals)
#       # display(fields)
#       #getting value of comments from fields
#       value = fields['comments']
#       #getting first value
#       values_str = value[0]
#       #separation from the value found - spo2
#       spo2_end_str = values_str.split('<spo2_end>: ')[1]
#       spo2_end = int(spo2_end_str.split()[0])
#       #finding and taking the length of the signal from the field
#       sig_len = fields['sig_len']
#       pleth=signals

#       result = [elem[0] for elem in pleth]
#       pleth_ = ','.join(map(str, result))
#       dict = {"ID": i,"PPG": pleth_,"Pleth":j,"SpO2": spo2_end}
#       rows_list2.append(dict)


# df2 = pd.DataFrame(rows_list2)
# df2.to_csv('database.csv', index=False)

df2.to_csv('NO_filtered-database.csv', index=False)

import matplotlib.pyplot as plt
third_column = df.iloc[:, 1]
column = df.iloc[:, 0]
p=df.iloc[:,:]
# print the third column
# print(third_column)
print(p)
print("------Stats------")
print("Total signals:",n)
print("Filtrered signals:",len(column))
print("Percent of clean and filtered signals:",((float)(len(column)/n)*100),"%")
print("------------------")
# display by signals Filtered
for i,row in enumerate(third_column):
    print(column[i])
    #10sec
    array_half = row[:10000]

    # array_half = row

    data = [float(x) for x in array_half]

    fig, ax = plt.subplots()
    ax.plot(data)
    ax.set_xlabel(column[i])
    ax.set_ylabel('Value')
    ax.set_title('Filtered signal')
    plt.show()



import matplotlib.pyplot as plt
third_column = df2.iloc[:, 1]
column = df2.iloc[:, 0]

print(third_column)
print(column)

# display by signals without filter
for i,row in enumerate(third_column):
    print(column[i])
    # print(row)
    my_array = row.split(',')
    # Checking empty object
    if my_array[-1] == "":
      my_array.pop()

    #10sec
    array_half = my_array[:10000]
    # array_half=my_array

    data = [float(y) for y in array_half]

    fig, ax = plt.subplots()
    ax.plot(data)
    ax.set_xlabel(column[i])
    ax.set_ylabel('Value')
    ax.set_title('Without Filter')
    plt.show()